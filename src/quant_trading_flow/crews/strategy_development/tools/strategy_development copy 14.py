import pandas as pd
import numpy as np
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    VotingRegressor,
    HistGradientBoostingRegressor,
    RandomForestClassifier,
    ExtraTreesRegressor,
    ExtraTreesClassifier,
    BaggingRegressor,
    AdaBoostRegressor,
    StackingRegressor,
)
from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge
from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, cross_val_score
from sklearn.metrics import (
    mean_absolute_error,
    accuracy_score,
    mean_squared_error,
    make_scorer,
)
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import (
    SelectFromModel,
    RFE,
    SelectKBest,
    f_regression,
    mutual_info_regression,
)
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import warnings

warnings.filterwarnings("ignore")

import ta
from scipy import stats
import xgboost as xgb
import lightgbm as lgb
from datetime import datetime, timedelta
import os
import json
import optuna
from optuna.samplers import TPESampler


class OptimizedStockPredictor:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.imputers = {}
        self.feature_importance = {}
        self.feature_selector = {}
        self.best_params = {}
        self.is_trained = False

    def create_expanded_features(self, df):
        """ÂàõÂª∫Êâ©Â±ïÁâπÂæÅÈõÜ - Â§ßÂπÖÂ¢ûÂä†ÁâπÂæÅÁª¥Â∫¶"""
        df = df.copy()

        # Á°Æ‰øùÊï∞ÊçÆÁ±ªÂûã
        for col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

        # Â°´ÂÖÖÁº∫Â§±ÂÄº
        df = df.ffill().bfill()

        # Âü∫Á°Ä‰ª∑Ê†ºÁâπÂæÅÊâ©Â±ï
        df["price_gap_ratio"] = (df["Open"] - df["Close"].shift(1)) / (
            df["Close"].shift(1) + 1e-8
        )
        df["intraday_power"] = (df["Close"] - df["Open"]) / (
            df["High"] - df["Low"] + 1e-8
        )
        df["close_strength"] = (df["Close"] - df["Low"]) / (
            df["High"] - df["Low"] + 1e-8
        )
        df["volatility_ratio"] = (df["High"] - df["Low"]) / (df["Close"] + 1e-8)
        df["price_range_ratio"] = (df["High"] - df["Low"]) / df["Open"]
        df["typical_price"] = (df["High"] + df["Low"] + df["Close"]) / 3
        df["median_price"] = (df["High"] + df["Low"]) / 2

        # Êâ©Â±ï‰ª∑Ê†º‰ΩçÁΩÆÁâπÂæÅ
        for period in [5, 10, 20, 50]:
            df[f"close_vs_high_{period}"] = (
                df["Close"] / df["High"].rolling(period).max() - 1
            )
            df[f"close_vs_low_{period}"] = (
                df["Close"] / df["Low"].rolling(period).min() - 1
            )
            df[f"close_percentile_{period}"] = (
                (df["Close"].rank() / len(df)).rolling(period).mean()
            )

        # Â§öÊó∂Èó¥Ê°ÜÊû∂Âä®ÈáèÊâ©Â±ï
        periods = [1, 2, 3, 5, 8, 13, 21, 34, 55, 89]
        for period in periods:
            # ‰ª∑Ê†ºÂä®Èáè
            df[f"return_{period}d"] = df["Close"].pct_change(period)
            df[f"log_return_{period}d"] = np.log(
                df["Close"] / df["Close"].shift(period)
            )
            df[f"high_{period}d"] = df["High"].rolling(period).max()
            df[f"low_{period}d"] = df["Low"].rolling(period).min()
            df[f"close_ma_{period}"] = df["Close"].rolling(period).mean()
            df[f"volume_ma_{period}"] = df["Volume"].rolling(period).mean()

            # Êàê‰∫§ÈáèÂä®Èáè
            df[f"volume_momentum_{period}"] = df["Volume"].pct_change(period)
            df[f"volume_ratio_{period}"] = df["Volume"] / (
                df[f"volume_ma_{period}"] + 1e-8
            )

            # ‰ª∑Ê†ºÂä†ÈÄüÂ∫¶ÁâπÂæÅ
            if period >= 3:
                df[f"price_accel_{period}"] = df[f"return_{period}d"] - df[
                    f"return_{period}d"
                ].shift(period)
            if period >= 5:
                df[f"price_jerk_{period}"] = df[f"price_accel_{period}"] - df[
                    f"price_accel_{period}"
                ].shift(period)

        # ‰ª∑Ê†ºÊ≥¢Âä®ÁâπÂæÅ
        for period in [5, 10, 20, 50]:
            df[f"volatility_{period}"] = df["Close"].pct_change().rolling(period).std()
            df[f"realized_volatility_{period}"] = np.sqrt(
                (df["Close"].pct_change() ** 2).rolling(period).sum()
            )
            df[f"close_zscore_{period}"] = (
                df["Close"] - df["Close"].rolling(period).mean()
            ) / (df["Close"].rolling(period).std() + 1e-8)

        # ‰ΩøÁî®taÂ∫ìËÆ°ÁÆóÂÖ®Èù¢ÁöÑÊäÄÊúØÊåáÊ†á
        try:
            # Êâ©Â±ïÂä®ÈáèÊåáÊ†áÁªÑ
            rsi_windows = [3, 6, 9, 14, 21, 26, 50]
            for window in rsi_windows:
                df[f"rsi_{window}"] = ta.momentum.RSIIndicator(
                    df["Close"], window=window
                ).rsi()

            df["stoch_rsi"] = ta.momentum.StochRSIIndicator(df["Close"]).stochrsi()
            df["tsi"] = ta.momentum.TSIIndicator(df["Close"]).tsi()
            df["uo"] = ta.momentum.UltimateOscillator(
                df["High"], df["Low"], df["Close"]
            ).ultimate_oscillator()
            df["williams_r"] = ta.momentum.WilliamsRIndicator(
                df["High"], df["Low"], df["Close"]
            ).williams_r()
            df["awesome_oscillator"] = ta.momentum.AwesomeOscillatorIndicator(
                df["High"], df["Low"]
            ).awesome_oscillator()
            df["kama"] = ta.momentum.KAMAIndicator(df["Close"]).kama()
            df["roc"] = ta.momentum.ROCIndicator(df["Close"]).roc()
            df["ppo"] = ta.momentum.PercentagePriceOscillator(df["Close"]).ppo()
            df["pvo"] = ta.momentum.PercentageVolumeOscillator(df["Volume"]).pvo()
            df["stoch"] = ta.momentum.StochasticOscillator(
                df["High"], df["Low"], df["Close"]
            ).stoch()
            df["stoch_signal"] = ta.momentum.StochasticOscillator(
                df["High"], df["Low"], df["Close"]
            ).stoch_signal()

            # Êâ©Â±ïË∂ãÂäøÊåáÊ†áÁªÑ
            df["macd"] = ta.trend.MACD(df["Close"]).macd()
            df["macd_signal"] = ta.trend.MACD(df["Close"]).macd_signal()
            df["macd_hist"] = ta.trend.MACD(df["Close"]).macd_diff()

            adx_windows = [14, 21]
            for window in adx_windows:
                df[f"adx_{window}"] = ta.trend.ADXIndicator(
                    df["High"], df["Low"], df["Close"], window=window
                ).adx()
                df[f"adx_pos_{window}"] = ta.trend.ADXIndicator(
                    df["High"], df["Low"], df["Close"], window=window
                ).adx_pos()
                df[f"adx_neg_{window}"] = ta.trend.ADXIndicator(
                    df["High"], df["Low"], df["Close"], window=window
                ).adx_neg()

            df["cci"] = ta.trend.CCIIndicator(df["High"], df["Low"], df["Close"]).cci()
            df["aroon_up"] = ta.trend.AroonIndicator(df["High"], df["Low"]).aroon_up()
            df["aroon_down"] = ta.trend.AroonIndicator(
                df["High"], df["Low"]
            ).aroon_down()
            df["aroon_osc"] = df["aroon_up"] - df["aroon_down"]
            df["vwap"] = ta.volume.VolumeWeightedAveragePrice(
                df["High"], df["Low"], df["Close"], df["Volume"]
            ).volume_weighted_average_price()

            # ‰∏ÄÁõÆÂùáË°°Ë°®ÊåáÊ†á
            df["ichimoku_a"] = ta.trend.IchimokuIndicator(
                df["High"], df["Low"]
            ).ichimoku_a()
            df["ichimoku_b"] = ta.trend.IchimokuIndicator(
                df["High"], df["Low"]
            ).ichimoku_b()
            df["ichimoku_base"] = ta.trend.IchimokuIndicator(
                df["High"], df["Low"]
            ).ichimoku_base_line()
            df["ichimoku_conversion"] = ta.trend.IchimokuIndicator(
                df["High"], df["Low"]
            ).ichimoku_conversion_line()

            # ÊäõÁâ©Á∫øËΩ¨ÂêëÊåáÊ†á
            df["psar"] = ta.trend.PSARIndicator(
                df["High"], df["Low"], df["Close"]
            ).psar()
            df["psar_up"] = ta.trend.PSARIndicator(
                df["High"], df["Low"], df["Close"]
            ).psar_up()
            df["psar_down"] = ta.trend.PSARIndicator(
                df["High"], df["Low"], df["Close"]
            ).psar_down()

            # Êâ©Â±ïÊ≥¢Âä®ÁéáÊåáÊ†áÁªÑ
            bb_windows = [10, 20, 30, 50]
            for window in bb_windows:
                df[f"bollinger_hband_{window}"] = ta.volatility.BollingerBands(
                    df["Close"], window=window
                ).bollinger_hband()
                df[f"bollinger_lband_{window}"] = ta.volatility.BollingerBands(
                    df["Close"], window=window
                ).bollinger_lband()
                df[f"bollinger_mband_{window}"] = ta.volatility.BollingerBands(
                    df["Close"], window=window
                ).bollinger_mavg()
                df[f"bollinger_pband_{window}"] = (
                    df["Close"] - df[f"bollinger_lband_{window}"]
                ) / (
                    df[f"bollinger_hband_{window}"]
                    - df[f"bollinger_lband_{window}"]
                    + 1e-8
                )
                df[f"bollinger_width_{window}"] = (
                    df[f"bollinger_hband_{window}"] - df[f"bollinger_lband_{window}"]
                ) / df["Close"]

            df["atr"] = ta.volatility.AverageTrueRange(
                df["High"], df["Low"], df["Close"]
            ).average_true_range()

            kc_windows = [10, 20]
            for window in kc_windows:
                df[f"keltner_channel_hband_{window}"] = ta.volatility.KeltnerChannel(
                    df["High"], df["Low"], df["Close"], window=window
                ).keltner_channel_hband()
                df[f"keltner_channel_lband_{window}"] = ta.volatility.KeltnerChannel(
                    df["High"], df["Low"], df["Close"], window=window
                ).keltner_channel_lband()
                df[f"keltner_channel_pband_{window}"] = (
                    df["Close"] - df[f"keltner_channel_lband_{window}"]
                ) / (
                    df[f"keltner_channel_hband_{window}"]
                    - df[f"keltner_channel_lband_{window}"]
                    + 1e-8
                )

            df["donchian_channel_hband"] = ta.volatility.DonchianChannel(
                df["High"], df["Low"], df["Close"]
            ).donchian_channel_hband()
            df["donchian_channel_lband"] = ta.volatility.DonchianChannel(
                df["High"], df["Low"], df["Close"]
            ).donchian_channel_lband()
            df["donchian_channel_mband"] = ta.volatility.DonchianChannel(
                df["High"], df["Low"], df["Close"]
            ).donchian_channel_mband()

            # Êâ©Â±ïÊàê‰∫§ÈáèÊåáÊ†áÁªÑ
            df["obv"] = ta.volume.OnBalanceVolumeIndicator(
                df["Close"], df["Volume"]
            ).on_balance_volume()
            df["cmf"] = ta.volume.ChaikinMoneyFlowIndicator(
                df["High"], df["Low"], df["Close"], df["Volume"]
            ).chaikin_money_flow()
            df["mfi"] = ta.volume.MFIIndicator(
                df["High"], df["Low"], df["Close"], df["Volume"]
            ).money_flow_index()
            df["volume_adi"] = ta.volume.AccDistIndexIndicator(
                df["High"], df["Low"], df["Close"], df["Volume"]
            ).acc_dist_index()
            df["volume_obv"] = ta.volume.OnBalanceVolumeIndicator(
                df["Close"], df["Volume"]
            ).on_balance_volume()
            df["volume_vpt"] = ta.volume.VolumePriceTrendIndicator(
                df["Close"], df["Volume"]
            ).volume_price_trend()
            df["eom"] = ta.volume.EaseOfMovementIndicator(
                df["High"], df["Low"], df["Volume"]
            ).ease_of_movement()
            df["volume_em"] = ta.volume.EaseOfMovementIndicator(
                df["High"], df["Low"], df["Volume"]
            ).ease_of_movement()
            df["volume_sma_em"] = ta.volume.EaseOfMovementIndicator(
                df["High"], df["Low"], df["Volume"]
            ).sma_ease_of_movement()
            df["volume_vwap"] = ta.volume.VolumeWeightedAveragePrice(
                df["High"], df["Low"], df["Close"], df["Volume"]
            ).volume_weighted_average_price()
            df["volume_fi"] = ta.volume.ForceIndexIndicator(
                df["Close"], df["Volume"]
            ).force_index()
            df["volume_nvi"] = ta.volume.NegativeVolumeIndexIndicator(
                df["Close"], df["Volume"]
            ).negative_volume_index()
            # PositiveVolumeIndexIndicatorÂú®taÂ∫ì‰∏≠‰∏çÂèØÁî®Ôºå‰ΩøÁî®ÂÖ∂‰ªñÊàê‰∫§ÈáèÊåáÊ†áÊõø‰ª£
            df["volume_pvi"] = ta.volume.OnBalanceVolumeIndicator(
                df["Close"], df["Volume"]
            ).on_balance_volume()

        except Exception as e:
            print(f"ÊäÄÊúØÊåáÊ†áËÆ°ÁÆóË≠¶Âëä: {e}")

        # ËµÑÈáëÊµÅÂêëÁâπÂæÅÊâ©Â±ï
        df["money_flow"] = df["Amount"] / (df["Volume"] + 1e-8)
        for period in [5, 10, 20, 50]:
            df[f"money_flow_ma_{period}"] = df["money_flow"].rolling(period).mean()
            df[f"money_flow_ratio_{period}"] = df["money_flow"] / (
                df[f"money_flow_ma_{period}"] + 1e-8
            )
            # Á°Æ‰øùvolume_ratio_{period}ÁâπÂæÅÂ≠òÂú®
            if f"volume_ratio_{period}" not in df.columns:
                df[f"volume_ma_{period}"] = df["Volume"].rolling(period).mean()
                df[f"volume_ratio_{period}"] = df["Volume"] / (
                    df[f"volume_ma_{period}"] + 1e-8
                )
            df[f"money_flow_volume_{period}"] = (
                df["money_flow"] * df[f"volume_ratio_{period}"]
            )

        # Ê∂®Ë∑åÂÅúÁâπÂæÅÂ¢ûÂº∫
        df["is_limit_up"] = (
            (abs(df["High"] - df["Low"]) / df["Close"] < 0.005) & (df["Change"] > 9.5)
        ).astype(int)
        df["is_limit_down"] = (
            (abs(df["High"] - df["Low"]) / df["Close"] < 0.005) & (df["Change"] < -9.5)
        ).astype(int)

        # ËøûÁª≠Ê∂®Ë∑åÂÅúËÆ°Êï∞
        df["consecutive_limit_up"] = 0
        df["consecutive_limit_down"] = 0

        up_count = 0
        down_count = 0
        for i in range(len(df)):
            if df["is_limit_up"].iloc[i] == 1:
                up_count += 1
                down_count = 0
            elif df["is_limit_down"].iloc[i] == 1:
                down_count += 1
                up_count = 0
            else:
                up_count = 0
                down_count = 0

            df.loc[df.index[i], "consecutive_limit_up"] = up_count
            df.loc[df.index[i], "consecutive_limit_down"] = down_count

        df["limit_strength"] = df["consecutive_limit_up"] - df["consecutive_limit_down"]

        # Ê∂®ÂÅúÊàê‰∫§ÈáèÁâπÂæÅ
        df["limit_up_volume_ratio"] = 0.0
        for i in range(len(df)):
            if df["is_limit_up"].iloc[i] == 1:
                vol_ma20 = (
                    df["volume_ma_20"].iloc[i]
                    if pd.notna(df["volume_ma_20"].iloc[i])
                    else df["Volume"].mean()
                )
                df.loc[df.index[i], "limit_up_volume_ratio"] = (
                    df["Volume"].iloc[i] / vol_ma20
                )

        # ÊîØÊíëÈòªÂäõÁâπÂæÅÊâ©Â±ï
        for period in [5, 10, 20, 50, 100, 200]:
            df[f"resistance_{period}"] = df["High"].rolling(period).max()
            df[f"support_{period}"] = df["Low"].rolling(period).min()
            df[f"dist_to_resistance_{period}"] = (
                df[f"resistance_{period}"] - df["Close"]
            ) / df["Close"]
            df[f"dist_to_support_{period}"] = (
                df["Close"] - df[f"support_{period}"]
            ) / df["Close"]
            df[f"resistance_strength_{period}"] = (
                df["Close"] - df[f"support_{period}"]
            ) / (df[f"resistance_{period}"] - df[f"support_{period}"] + 1e-8)

        # Á™ÅÁ†¥ÁâπÂæÅÊâ©Â±ï
        for period in [5, 10, 20, 50]:
            df[f"breakout_high_{period}"] = (
                df["Close"] > df[f"resistance_{period}"]
            ).astype(int)
            df[f"breakout_low_{period}"] = (
                df["Close"] < df[f"support_{period}"]
            ).astype(int)
            df[f"near_breakout_high_{period}"] = (
                (df["Close"] / df[f"resistance_{period}"] - 1) > -0.02
            ).astype(int)
            df[f"near_breakout_low_{period}"] = (
                (df["Close"] / df[f"support_{period}"] - 1) < 0.02
            ).astype(int)

        # Ë∂ãÂäøÂº∫Â∫¶ÁâπÂæÅÊâ©Â±ï
        for period in [5, 10, 20, 50]:
            df[f"trend_strength_{period}"] = (
                df["Close"] - df["Close"].rolling(period).mean()
            ) / (df["Close"].rolling(period).std() + 1e-8)
            df[f"trend_direction_{period}"] = np.sign(
                df["Close"] - df["Close"].rolling(period).mean()
            )
            df[f"trend_acceleration_{period}"] = df[f"trend_strength_{period}"].diff()

        # ÂèçËΩ¨‰ø°Âè∑Êâ©Â±ï
        for rsi_period in [6, 14, 21]:
            df[f"rsi_overbought_{rsi_period}"] = (df[f"rsi_{rsi_period}"] > 70).astype(
                int
            )
            df[f"rsi_oversold_{rsi_period}"] = (df[f"rsi_{rsi_period}"] < 30).astype(
                int
            )
            df[f"rsi_divergence_{rsi_period}"] = (
                df[f"rsi_{rsi_period}"] - df[f"rsi_{rsi_period}"].shift(5)
            ) - (df["Close"] - df["Close"].shift(5)) / df["Close"].shift(5) * 100

        df["williams_overbought"] = (df["williams_r"] > -20).astype(int)
        df["williams_oversold"] = (df["williams_r"] < -80).astype(int)
        df["cci_overbought"] = (df["cci"] > 100).astype(int)
        df["cci_oversold"] = (df["cci"] < -100).astype(int)
        df["stoch_overbought"] = (df["stoch"] > 80).astype(int)
        df["stoch_oversold"] = (df["stoch"] < 20).astype(int)

        # ‰ª∑Ê†ºÊ®°ÂºèÁâπÂæÅÊâ©Â±ï
        df["higher_high"] = (df["High"] > df["High"].shift(1)).astype(int)
        df["higher_low"] = (df["Low"] > df["Low"].shift(1)).astype(int)
        df["lower_high"] = (df["High"] < df["High"].shift(1)).astype(int)
        df["lower_low"] = (df["Low"] < df["Low"].shift(1)).astype(int)
        df["inside_bar"] = (
            (df["High"] < df["High"].shift(1)) & (df["Low"] > df["Low"].shift(1))
        ).astype(int)
        df["outside_bar"] = (
            (df["High"] > df["High"].shift(1)) & (df["Low"] < df["Low"].shift(1))
        ).astype(int)

        # Ê∑ªÂä†Êõ¥Â§ö‰ª∑Ê†ºÊ®°Âºè
        df["bullish_engulfing"] = (
            (df["Close"] > df["Open"])
            & (df["Close"].shift(1) < df["Open"].shift(1))
            & (df["Close"] > df["Open"].shift(1))
            & (df["Open"] < df["Close"].shift(1))
        ).astype(int)

        df["bearish_engulfing"] = (
            (df["Close"] < df["Open"])
            & (df["Close"].shift(1) > df["Open"].shift(1))
            & (df["Close"] < df["Open"].shift(1))
            & (df["Open"] > df["Close"].shift(1))
        ).astype(int)

        df["hammer"] = (
            (df["Close"] > df["Open"])
            & ((df["Close"] - df["Low"]) > 2 * (df["High"] - df["Close"]))
            & ((df["Open"] - df["Low"]) > 2 * (df["High"] - df["Open"]))
        ).astype(int)

        df["shooting_star"] = (
            (df["Close"] < df["Open"])
            & ((df["High"] - df["Close"]) > 2 * (df["Close"] - df["Low"]))
            & ((df["High"] - df["Open"]) > 2 * (df["Open"] - df["Low"]))
        ).astype(int)

        # È´òÁ∫ß‰∫§‰∫íÁâπÂæÅÊâ©Â±ï
        df["rsi_volume_power"] = df["rsi_14"] * df["volume_ratio_5"]
        df["macd_volume_power"] = df["macd"] * df["volume_ratio_5"]
        df["trend_volume_power"] = df["trend_strength_5"] * df["volume_ratio_5"]
        df["momentum_composite"] = (
            (df["rsi_14"] / 100) * df["macd"] * df["volume_ratio_5"]
        )
        df["breakout_momentum"] = (
            df["breakout_high_10"] * df["volume_ratio_5"] * df["trend_strength_10"]
        )
        df["limit_momentum"] = (
            df["consecutive_limit_up"] * df["volume_ratio_5"] * (df["rsi_14"] / 100)
        )

        # Ê∑ªÂä†Êõ¥Â§ö‰∫§‰∫íÁâπÂæÅ
        df["adx_volume_power"] = df["adx_14"] * df["volume_ratio_5"]
        df["cci_volume_power"] = df["cci"] * df["volume_ratio_5"]
        df["bollinger_volume_power"] = df["bollinger_pband_20"] * df["volume_ratio_5"]
        df["volatility_volume_power"] = df["volatility_20"] * df["volume_ratio_5"]
        df["money_flow_power"] = df["money_flow_ratio_5"] * df["volume_ratio_5"]
        df["trend_momentum_composite"] = (
            df["trend_strength_10"] * df["momentum_composite"]
        )

        # ‰ª∑Ê†º‰ΩçÁΩÆÁâπÂæÅÊâ©Â±ï
        df["close_vs_vwap"] = df["Close"] / df["vwap"] - 1
        df["close_vs_bollinger"] = (df["Close"] - df["bollinger_lband_20"]) / (
            df["bollinger_hband_20"] - df["bollinger_lband_20"] + 1e-8
        )
        df["close_vs_keltner"] = (df["Close"] - df["keltner_channel_lband_20"]) / (
            df["keltner_channel_hband_20"] - df["keltner_channel_lband_20"] + 1e-8
        )
        df["close_vs_ichimoku"] = (df["Close"] - df["ichimoku_base"]) / (
            df["ichimoku_conversion"] + 1e-8
        )
        df["close_vs_psar"] = (df["Close"] - df["psar"]) / df["Close"]

        # Â∏ÇÂú∫Áä∂ÊÄÅÁâπÂæÅ
        # Á°Æ‰øùclose_ma_20ÁâπÂæÅÂ≠òÂú®
        if "close_ma_20" not in df.columns:
            df["close_ma_20"] = df["Close"].rolling(20).mean()
        df["market_trend"] = (df["Close"] > df["close_ma_20"]).astype(int)

        # Á°Æ‰øùclose_ma_5ÁâπÂæÅÂ≠òÂú®
        if "close_ma_5" not in df.columns:
            df["close_ma_5"] = df["Close"].rolling(5).mean()
        df["market_momentum"] = (df["close_ma_5"] > df["close_ma_20"]).astype(int)
        df["market_volatility"] = df["High"].rolling(20).std() / (
            df["Close"].rolling(20).mean() + 1e-8
        )
        df["market_regime"] = np.where(
            df["market_volatility"] > df["market_volatility"].quantile(0.7),
            2,
            np.where(
                df["market_volatility"] < df["market_volatility"].quantile(0.3), 0, 1
            ),
        )

        # Êó∂Èó¥ÁâπÂæÅÊâ©Â±ï
        if "Date" in df.columns:
            df["day_of_week"] = pd.to_datetime(df["Date"]).dt.dayofweek
            df["month"] = pd.to_datetime(df["Date"]).dt.month
            df["quarter"] = pd.to_datetime(df["Date"]).dt.quarter
            df["is_month_start"] = pd.to_datetime(df["Date"]).dt.is_month_start.astype(
                int
            )
            df["is_month_end"] = pd.to_datetime(df["Date"]).dt.is_month_end.astype(int)
            df["is_quarter_start"] = pd.to_datetime(
                df["Date"]
            ).dt.is_quarter_start.astype(int)
            df["is_quarter_end"] = pd.to_datetime(df["Date"]).dt.is_quarter_end.astype(
                int
            )
            df["day_of_year"] = pd.to_datetime(df["Date"]).dt.dayofyear
            df["week_of_year"] = pd.to_datetime(df["Date"]).dt.isocalendar().week

        # Âë®ÊúüÁâπÂæÅ
        df["sin_day"] = np.sin(2 * np.pi * df["day_of_week"] / 7)
        df["cos_day"] = np.cos(2 * np.pi * df["day_of_week"] / 7)
        df["sin_month"] = np.sin(2 * np.pi * df["month"] / 12)
        df["cos_month"] = np.cos(2 * np.pi * df["month"] / 12)

        # ÁªüËÆ°ÁâπÂæÅ
        df["skewness_5"] = df["Close"].rolling(5).skew()
        df["kurtosis_5"] = df["Close"].rolling(5).kurt()  # ‰ΩøÁî®kurt()ËÄå‰∏çÊòØkurtosis()
        df["skewness_20"] = df["Close"].rolling(20).skew()
        df["kurtosis_20"] = df["Close"].rolling(20).kurt()  # ‰ΩøÁî®kurt()ËÄå‰∏çÊòØkurtosis()

        # Áõ∏ÂÖ≥ÊÄßÁâπÂæÅ
        for col in ["Volume", "Amount", "TurnoverRate"]:
            df[f"close_{col}_corr_10"] = df["Close"].rolling(10).corr(df[col])

        # ÂÜçÊ¨°Â°´ÂÖÖÁº∫Â§±ÂÄº
        df = df.ffill().bfill().fillna(0)

        return df

    def create_enhanced_targets(self, df):
        """ÂàõÂª∫Â¢ûÂº∫ÁõÆÊ†áÂèòÈáè"""
        df = df.copy()

        # Âü∫Á°ÄÁõÆÊ†á
        df["target_next_low"] = df["Low"].shift(-1)
        df["target_next_high"] = df["High"].shift(-1)
        df["target_next_next_high"] = df["High"].shift(-2)
        df["target_next_next_low"] = df["Low"].shift(-2)

        # Ê∂®Ë∑åÁõÆÊ†áÊâ©Â±ï
        df["target_next_next_up"] = (df["Close"].shift(-2) > df["Close"]).astype(int)
        df["target_next_up"] = (df["Close"].shift(-1) > df["Close"]).astype(int)

        # Ê∂®ÂπÖÁõÆÊ†áÊâ©Â±ï
        for threshold in [1, 2, 3, 5, 7, 10, 15]:
            df[f"target_up_{threshold}pct"] = (
                (df["Close"].shift(-1) / df["Close"] - 1) > threshold / 100
            ).astype(int)
            df[f"target_down_{threshold}pct"] = (
                (df["Close"].shift(-1) / df["Close"] - 1) < -threshold / 100
            ).astype(int)

        # Ê∂®ÂÅúÁõÆÊ†á
        df["target_limit_up"] = (
            (
                abs(df["High"].shift(-1) - df["Low"].shift(-1)) / df["Close"].shift(-1)
                < 0.005
            )
            & (df["Change"].shift(-1) > 9.5)
        ).astype(int)
        df["target_limit_up_next_next"] = (
            (
                abs(df["High"].shift(-2) - df["Low"].shift(-2)) / df["Close"].shift(-2)
                < 0.005
            )
            & (df["Change"].shift(-2) > 9.5)
        ).astype(int)

        # ËøûÁª≠Ê∂®ÂÅúÁõÆÊ†á
        df["target_consecutive_limit"] = (
            (df["target_limit_up"] == 1) & (df["is_limit_up"] == 1)
        ).astype(int)

        # ‰ª∑Ê†ºÂå∫Èó¥ÁõÆÊ†á
        df["target_price_range"] = (df["High"].shift(-1) - df["Low"].shift(-1)) / df[
            "Close"
        ]
        df["target_volatility"] = df["Close"].pct_change().shift(-1).abs()

        return df

    def prepare_optimized_features(self, df):
        """ÂáÜÂ§á‰ºòÂåñÁâπÂæÅÈõÜ - ‰ΩøÁî®ÁâπÂæÅÈÄâÊã©"""
        # Ëé∑ÂèñÊâÄÊúâÂèØËÉΩÁöÑÁâπÂæÅ
        all_possible_features = [
            # Âü∫Á°ÄÁâπÂæÅ
            "Open",
            "High",
            "Low",
            "Close",
            "Volume",
            "Amount",
            "Change",
            "Amplitude",
            "TurnoverRate",
            # Ë°çÁîü‰ª∑Ê†ºÁâπÂæÅ
            "price_gap_ratio",
            "intraday_power",
            "close_strength",
            "volatility_ratio",
            "price_range_ratio",
            "typical_price",
            "median_price",
            # Âä®ÈáèÁâπÂæÅ
            "rsi_3",
            "rsi_6",
            "rsi_9",
            "rsi_14",
            "rsi_21",
            "rsi_26",
            "rsi_50",
            "stoch_rsi",
            "tsi",
            "uo",
            "williams_r",
            "awesome_oscillator",
            "kama",
            "roc",
            "ppo",
            "pvo",
            "stoch",
            "stoch_signal",
            # Ë∂ãÂäøÁâπÂæÅ
            "macd",
            "macd_signal",
            "macd_hist",
            "adx_14",
            "adx_21",
            "adx_pos_14",
            "adx_neg_14",
            "cci",
            "aroon_up",
            "aroon_down",
            "aroon_osc",
            "vwap",
            "ichimoku_a",
            "ichimoku_b",
            "ichimoku_base",
            "ichimoku_conversion",
            "psar",
            "psar_up",
            "psar_down",
            # Êàê‰∫§ÈáèÁâπÂæÅ
            "obv",
            "cmf",
            "mfi",
            "volume_adi",
            "volume_obv",
            "volume_vpt",
            "eom",
            "volume_em",
            "volume_sma_em",
            "volume_vwap",
            "volume_fi",
            "volume_nvi",
            "volume_pvi",
            # Â∏ÇÂú∫Áä∂ÊÄÅ
            "market_trend",
            "market_momentum",
            "market_volatility",
            "market_regime",
            # Êó∂Èó¥ÁâπÂæÅ
            "day_of_week",
            "month",
            "quarter",
            "is_month_start",
            "is_month_end",
            "is_quarter_start",
            "is_quarter_end",
            "day_of_year",
            "week_of_year",
            "sin_day",
            "cos_day",
            "sin_month",
            "cos_month",
            # ÁªüËÆ°ÁâπÂæÅ
            "skewness_5",
            "kurtosis_5",
            "skewness_20",
            "kurtosis_20",
        ]

        # Ê∑ªÂä†Âä®ÊÄÅÁâπÂæÅ
        for period in [1, 2, 3, 5, 8, 13, 20, 21, 34, 50, 55, 89]:
            all_possible_features.extend(
                [
                    f"return_{period}d",
                    f"log_return_{period}d",
                    f"high_{period}d",
                    f"low_{period}d",
                    f"close_ma_{period}",
                    f"volume_ma_{period}",
                    f"volume_momentum_{period}",
                    f"volume_ratio_{period}",
                ]
            )

        for period in [5, 10, 20, 50]:
            all_possible_features.extend(
                [
                    f"close_vs_high_{period}",
                    f"close_vs_low_{period}",
                    f"close_percentile_{period}",
                    f"volatility_{period}",
                    f"realized_volatility_{period}",
                    f"close_zscore_{period}",
                    f"resistance_{period}",
                    f"support_{period}",
                    f"dist_to_resistance_{period}",
                    f"dist_to_support_{period}",
                    f"resistance_strength_{period}",
                    f"breakout_high_{period}",
                    f"breakout_low_{period}",
                    f"near_breakout_high_{period}",
                    f"near_breakout_low_{period}",
                    f"trend_strength_{period}",
                    f"trend_direction_{period}",
                    f"trend_acceleration_{period}",
                    f"money_flow_ma_{period}",
                    f"money_flow_ratio_{period}",
                    f"money_flow_volume_{period}",
                ]
            )

        for window in [10, 20, 30, 50]:
            all_possible_features.extend(
                [
                    f"bollinger_hband_{window}",
                    f"bollinger_lband_{window}",
                    f"bollinger_mband_{window}",
                    f"bollinger_pband_{window}",
                    f"bollinger_width_{window}",
                    f"keltner_channel_hband_{window}",
                    f"keltner_channel_lband_{window}",
                    f"keltner_channel_pband_{window}",
                ]
            )

        # Âè™ÈÄâÊã©Â≠òÂú®ÁöÑÁâπÂæÅ
        available_features = [f for f in all_possible_features if f in df.columns]

        # Â¶ÇÊûúÁâπÂæÅÂ§™Â§öÔºåËøõË°åÂàùÊ≠•Á≠õÈÄâ
        if len(available_features) > 200:
            # ‰ΩøÁî®ÈöèÊú∫Ê£ÆÊûóËøõË°åÁâπÂæÅÈáçË¶ÅÊÄßÊéíÂ∫è
            X_temp = df[available_features].fillna(0)
            y_temp = df["Close"].shift(-1).fillna(method="ffill")

            # ÁßªÈô§Êó†ÈôêÂÄº
            X_temp = X_temp.replace([np.inf, -np.inf], 0)

            rf_selector = RandomForestRegressor(
                n_estimators=100, random_state=42, n_jobs=-1
            )
            rf_selector.fit(X_temp, y_temp)

            # ÈÄâÊã©ÈáçË¶ÅÊÄßÊúÄÈ´òÁöÑ200‰∏™ÁâπÂæÅ
            importances = rf_selector.feature_importances_
            feature_importance_df = pd.DataFrame(
                {"feature": available_features, "importance": importances}
            ).sort_values("importance", ascending=False)

            selected_features = feature_importance_df.head(200)["feature"].tolist()
        else:
            selected_features = available_features

        features_df = df[selected_features].copy()

        # ÊúÄÁªàÁº∫Â§±ÂÄºÂ§ÑÁêÜ
        features_df = features_df.ffill().bfill().fillna(0)

        return features_df

    def optimize_hyperparameters(self, X, y, problem_type="regression"):
        """‰ΩøÁî®OptunaËøõË°åË∂ÖÂèÇÊï∞‰ºòÂåñ"""

        def objective(trial):
            if problem_type == "regression":
                model_type = trial.suggest_categorical(
                    "model_type", ["rf", "xgb", "lgb", "et"]
                )

                if model_type == "rf":
                    params = {
                        "n_estimators": trial.suggest_int("rf_n_estimators", 100, 1000),
                        "max_depth": trial.suggest_int("rf_max_depth", 5, 30),
                        "min_samples_split": trial.suggest_int(
                            "rf_min_samples_split", 2, 20
                        ),
                        "min_samples_leaf": trial.suggest_int(
                            "rf_min_samples_leaf", 1, 10
                        ),
                        "max_features": trial.suggest_categorical(
                            "rf_max_features", ["sqrt", "log2", None]
                        ),
                    }
                    model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)

                elif model_type == "xgb":
                    params = {
                        "n_estimators": trial.suggest_int(
                            "xgb_n_estimators", 100, 1000
                        ),
                        "max_depth": trial.suggest_int("xgb_max_depth", 3, 15),
                        "learning_rate": trial.suggest_float(
                            "xgb_learning_rate", 0.01, 0.3
                        ),
                        "subsample": trial.suggest_float("xgb_subsample", 0.6, 1.0),
                        "colsample_bytree": trial.suggest_float(
                            "xgb_colsample_bytree", 0.6, 1.0
                        ),
                        "reg_alpha": trial.suggest_float("xgb_reg_alpha", 0, 1),
                        "reg_lambda": trial.suggest_float("xgb_reg_lambda", 0, 1),
                    }
                    model = xgb.XGBRegressor(**params, random_state=42, n_jobs=-1)

                elif model_type == "lgb":
                    params = {
                        "n_estimators": trial.suggest_int(
                            "lgb_n_estimators", 100, 1000
                        ),
                        "max_depth": trial.suggest_int("lgb_max_depth", 3, 15),
                        "learning_rate": trial.suggest_float(
                            "lgb_learning_rate", 0.01, 0.3
                        ),
                        "num_leaves": trial.suggest_int("lgb_num_leaves", 20, 100),
                        "subsample": trial.suggest_float("lgb_subsample", 0.6, 1.0),
                        "colsample_bytree": trial.suggest_float(
                            "lgb_colsample_bytree", 0.6, 1.0
                        ),
                        "reg_alpha": trial.suggest_float("lgb_reg_alpha", 0, 1),
                        "reg_lambda": trial.suggest_float("lgb_reg_lambda", 0, 1),
                    }
                    model = lgb.LGBMRegressor(
                        **params, random_state=42, n_jobs=-1, verbose=-1
                    )

                else:  # et
                    params = {
                        "n_estimators": trial.suggest_int("et_n_estimators", 100, 1000),
                        "max_depth": trial.suggest_int("et_max_depth", 5, 30),
                        "min_samples_split": trial.suggest_int(
                            "et_min_samples_split", 2, 20
                        ),
                        "min_samples_leaf": trial.suggest_int(
                            "et_min_samples_leaf", 1, 10
                        ),
                        "max_features": trial.suggest_categorical(
                            "et_max_features", ["sqrt", "log2", None]
                        ),
                    }
                    model = ExtraTreesRegressor(**params, random_state=42, n_jobs=-1)

            else:  # classification
                model_type = trial.suggest_categorical(
                    "model_type", ["rf", "xgb", "lgb", "et"]
                )

                if model_type == "rf":
                    params = {
                        "n_estimators": trial.suggest_int("rf_n_estimators", 100, 1000),
                        "max_depth": trial.suggest_int("rf_max_depth", 5, 30),
                        "min_samples_split": trial.suggest_int(
                            "rf_min_samples_split", 2, 20
                        ),
                        "min_samples_leaf": trial.suggest_int(
                            "rf_min_samples_leaf", 1, 10
                        ),
                        "max_features": trial.suggest_categorical(
                            "rf_max_features", ["sqrt", "log2", None]
                        ),
                    }
                    model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)

                elif model_type == "xgb":
                    params = {
                        "n_estimators": trial.suggest_int(
                            "xgb_n_estimators", 100, 1000
                        ),
                        "max_depth": trial.suggest_int("xgb_max_depth", 3, 15),
                        "learning_rate": trial.suggest_float(
                            "xgb_learning_rate", 0.01, 0.3
                        ),
                        "subsample": trial.suggest_float("xgb_subsample", 0.6, 1.0),
                        "colsample_bytree": trial.suggest_float(
                            "xgb_colsample_bytree", 0.6, 1.0
                        ),
                        "reg_alpha": trial.suggest_float("xgb_reg_alpha", 0, 1),
                        "reg_lambda": trial.suggest_float("xgb_reg_lambda", 0, 1),
                    }
                    model = xgb.XGBClassifier(**params, random_state=42, n_jobs=-1)

                elif model_type == "lgb":
                    params = {
                        "n_estimators": trial.suggest_int(
                            "lgb_n_estimators", 100, 1000
                        ),
                        "max_depth": trial.suggest_int("lgb_max_depth", 3, 15),
                        "learning_rate": trial.suggest_float(
                            "lgb_learning_rate", 0.01, 0.3
                        ),
                        "num_leaves": trial.suggest_int("lgb_num_leaves", 20, 100),
                        "subsample": trial.suggest_float("lgb_subsample", 0.6, 1.0),
                        "colsample_bytree": trial.suggest_float(
                            "lgb_colsample_bytree", 0.6, 1.0
                        ),
                        "reg_alpha": trial.suggest_float("lgb_reg_alpha", 0, 1),
                        "reg_lambda": trial.suggest_float("lgb_reg_lambda", 0, 1),
                    }
                    model = lgb.LGBMClassifier(
                        **params, random_state=42, n_jobs=-1, verbose=-1
                    )

                else:  # et
                    params = {
                        "n_estimators": trial.suggest_int("et_n_estimators", 100, 1000),
                        "max_depth": trial.suggest_int("et_max_depth", 5, 30),
                        "min_samples_split": trial.suggest_int(
                            "et_min_samples_split", 2, 20
                        ),
                        "min_samples_leaf": trial.suggest_int(
                            "et_min_samples_leaf", 1, 10
                        ),
                        "max_features": trial.suggest_categorical(
                            "et_max_features", ["sqrt", "log2", None]
                        ),
                    }
                    model = ExtraTreesClassifier(**params, random_state=42, n_jobs=-1)

            # ‰∫§ÂèâÈ™åËØÅ
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []

            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                try:
                    model.fit(X_train, y_train)

                    if problem_type == "regression":
                        y_pred = model.predict(X_val)
                        # ‰ΩøÁî®ÂØπÁß∞MAPE
                        smape = 2.0 * np.mean(
                            np.abs(y_val - y_pred)
                            / (np.abs(y_val) + np.abs(y_pred) + 1e-8)
                        )
                        score = 1 - smape
                    else:
                        y_pred = model.predict(X_val)
                        score = accuracy_score(y_val, y_pred)

                    scores.append(score)

                except:
                    continue

            return np.mean(scores) if scores else 0

        study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=42))
        study.optimize(objective, n_trials=50, show_progress_bar=True)

        return study.best_params, study.best_value

    def train_optimized_models(self, X, y_dict):
        """ËÆ≠ÁªÉ‰ºòÂåñÊ®°Âûã - ‰ΩøÁî®Ë∂ÖÂèÇÊï∞‰ºòÂåñÂíåÈ´òÁ∫ßÈõÜÊàê"""
        print("ËÆ≠ÁªÉ‰ºòÂåñÈ¢ÑÊµãÊ®°Âûã...")

        for target_name, y in y_dict.items():
            if len(y) < 100:
                print(f"Ë∑≥Ëøá {target_name}: Êï∞ÊçÆÈáè‰∏çË∂≥")
                continue

            # Ê∏ÖÁêÜÊï∞ÊçÆ
            mask = ~(y.isna() | X.isna().any(axis=1))
            X_clean = X[mask]
            y_clean = y[mask]

            if len(X_clean) < 50:
                print(f"Ë∑≥Ëøá {target_name}: Ê∏ÖÊ¥óÂêéÊï∞ÊçÆÈáè‰∏çË∂≥")
                continue

            # ÂàõÂª∫imputer
            imputer = KNNImputer(n_neighbors=5)
            X_imputed = imputer.fit_transform(X_clean)

            # Âà§Êñ≠ÈóÆÈ¢òÁ±ªÂûã
            is_classification = target_name in [
                "target_next_next_up",
                "target_next_up",
                "target_up_1pct",
                "target_up_2pct",
                "target_up_3pct",
                "target_up_5pct",
                "target_up_7pct",
                "target_up_10pct",
                "target_up_15pct",
                "target_down_1pct",
                "target_down_2pct",
                "target_down_3pct",
                "target_down_5pct",
                "target_down_7pct",
                "target_down_10pct",
                "target_down_15pct",
                "target_limit_up",
                "target_limit_up_next_next",
                "target_consecutive_limit",
            ]

            print(f"üîß ‰ºòÂåñË∂ÖÂèÇÊï∞ {target_name}...")
            best_params, best_score = self.optimize_hyperparameters(
                X_imputed,
                y_clean,
                "classification" if is_classification else "regression",
            )

            self.best_params[target_name] = best_params
            print(f"‚úÖ {target_name} ÊúÄ‰Ω≥ÂèÇÊï∞: {best_params}")
            print(f"‚úÖ {target_name} ÊúÄ‰Ω≥ÂæóÂàÜ: {best_score:.4f}")

            # Ê†πÊçÆ‰ºòÂåñÁªìÊûúÂàõÂª∫Ê®°Âûã
            model_type = best_params["model_type"]

            if is_classification:
                if model_type == "rf":
                    model = RandomForestClassifier(
                        n_estimators=best_params.get("rf_n_estimators", 500),
                        max_depth=best_params.get("rf_max_depth", 20),
                        min_samples_split=best_params.get("rf_min_samples_split", 5),
                        min_samples_leaf=best_params.get("rf_min_samples_leaf", 3),
                        max_features=best_params.get("rf_max_features", "sqrt"),
                        random_state=42,
                        n_jobs=-1,
                    )
                elif model_type == "xgb":
                    model = xgb.XGBClassifier(
                        n_estimators=best_params.get("xgb_n_estimators", 500),
                        max_depth=best_params.get("xgb_max_depth", 8),
                        learning_rate=best_params.get("xgb_learning_rate", 0.1),
                        subsample=best_params.get("xgb_subsample", 0.8),
                        colsample_bytree=best_params.get("xgb_colsample_bytree", 0.8),
                        reg_alpha=best_params.get("xgb_reg_alpha", 0.1),
                        reg_lambda=best_params.get("xgb_reg_lambda", 0.1),
                        random_state=42,
                        n_jobs=-1,
                    )
                elif model_type == "lgb":
                    model = lgb.LGBMClassifier(
                        n_estimators=best_params.get("lgb_n_estimators", 500),
                        max_depth=best_params.get("lgb_max_depth", 8),
                        learning_rate=best_params.get("lgb_learning_rate", 0.1),
                        num_leaves=best_params.get("lgb_num_leaves", 31),
                        subsample=best_params.get("lgb_subsample", 0.8),
                        colsample_bytree=best_params.get("lgb_colsample_bytree", 0.8),
                        reg_alpha=best_params.get("lgb_reg_alpha", 0.1),
                        reg_lambda=best_params.get("lgb_reg_lambda", 0.1),
                        random_state=42,
                        n_jobs=-1,
                        verbose=-1,
                    )
                else:  # et
                    model = ExtraTreesClassifier(
                        n_estimators=best_params.get("et_n_estimators", 500),
                        max_depth=best_params.get("et_max_depth", 20),
                        min_samples_split=best_params.get("et_min_samples_split", 5),
                        min_samples_leaf=best_params.get("et_min_samples_leaf", 3),
                        max_features=best_params.get("et_max_features", "sqrt"),
                        random_state=42,
                        n_jobs=-1,
                    )
            else:
                if model_type == "rf":
                    model = RandomForestRegressor(
                        n_estimators=best_params.get("rf_n_estimators", 500),
                        max_depth=best_params.get("rf_max_depth", 20),
                        min_samples_split=best_params.get("rf_min_samples_split", 5),
                        min_samples_leaf=best_params.get("rf_min_samples_leaf", 3),
                        max_features=best_params.get("rf_max_features", "sqrt"),
                        random_state=42,
                        n_jobs=-1,
                    )
                elif model_type == "xgb":
                    model = xgb.XGBRegressor(
                        n_estimators=best_params.get("xgb_n_estimators", 500),
                        max_depth=best_params.get("xgb_max_depth", 8),
                        learning_rate=best_params.get("xgb_learning_rate", 0.1),
                        subsample=best_params.get("xgb_subsample", 0.8),
                        colsample_bytree=best_params.get("xgb_colsample_bytree", 0.8),
                        reg_alpha=best_params.get("xgb_reg_alpha", 0.1),
                        reg_lambda=best_params.get("xgb_reg_lambda", 0.1),
                        random_state=42,
                        n_jobs=-1,
                    )
                elif model_type == "lgb":
                    model = lgb.LGBMRegressor(
                        n_estimators=best_params.get("lgb_n_estimators", 500),
                        max_depth=best_params.get("lgb_max_depth", 8),
                        learning_rate=best_params.get("lgb_learning_rate", 0.1),
                        num_leaves=best_params.get("lgb_num_leaves", 31),
                        subsample=best_params.get("lgb_subsample", 0.8),
                        colsample_bytree=best_params.get("lgb_colsample_bytree", 0.8),
                        reg_alpha=best_params.get("lgb_reg_alpha", 0.1),
                        reg_lambda=best_params.get("lgb_reg_lambda", 0.1),
                        random_state=42,
                        n_jobs=-1,
                        verbose=-1,
                    )
                else:  # et
                    model = ExtraTreesRegressor(
                        n_estimators=best_params.get("et_n_estimators", 500),
                        max_depth=best_params.get("et_max_depth", 20),
                        min_samples_split=best_params.get("et_min_samples_split", 5),
                        min_samples_leaf=best_params.get("et_min_samples_leaf", 3),
                        max_features=best_params.get("et_max_features", "sqrt"),
                        random_state=42,
                        n_jobs=-1,
                    )

            # ÁâπÂæÅÈÄâÊã©
            feature_selector = SelectFromModel(
                (
                    RandomForestRegressor(n_estimators=100, random_state=42)
                    if not is_classification
                    else RandomForestClassifier(n_estimators=100, random_state=42)
                ),
                threshold="median",
            )
            X_selected = feature_selector.fit_transform(X_imputed, y_clean)

            # Êó∂Èó¥Â∫èÂàó‰∫§ÂèâÈ™åËØÅ
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []

            for train_idx, val_idx in tscv.split(X_selected):
                X_train, X_val = X_selected[train_idx], X_selected[val_idx]
                y_train, y_val = y_clean.iloc[train_idx], y_clean.iloc[val_idx]

                try:
                    # ÁâπÂæÅÊ†áÂáÜÂåñ
                    scaler = QuantileTransformer(
                        output_distribution="normal", random_state=42
                    )
                    X_train_scaled = scaler.fit_transform(X_train)
                    X_val_scaled = scaler.transform(X_val)

                    model.fit(X_train_scaled, y_train)

                    if is_classification:
                        y_pred = model.predict(X_val_scaled)
                        score = accuracy_score(y_val, y_pred)
                    else:
                        y_pred = model.predict(X_val_scaled)
                        # ‰ΩøÁî®ÂØπÁß∞MAPE
                        smape = 2.0 * np.mean(
                            np.abs(y_val - y_pred)
                            / (np.abs(y_val) + np.abs(y_pred) + 1e-8)
                        )
                        score = 1 - smape

                    scores.append(score)

                except Exception as e:
                    print(f"‰∫§ÂèâÈ™åËØÅÈîôËØØ {target_name}: {e}")
                    continue

            if scores and np.mean(scores) > 0:
                # ÊúÄÁªàÊ®°ÂûãËÆ≠ÁªÉ
                scaler = QuantileTransformer(
                    output_distribution="normal", random_state=42
                )
                X_clean_scaled = scaler.fit_transform(X_selected)

                model.fit(X_clean_scaled, y_clean)

                self.models[target_name] = {
                    "model": model,
                    "scaler": scaler,
                    "imputer": imputer,
                    "feature_selector": feature_selector,
                    "cv_score": np.mean(scores),
                    "cv_std": np.std(scores),
                }

                score_type = "ÂáÜÁ°ÆÁéá" if is_classification else "R¬≤ÂæóÂàÜ"
                print(
                    f"‚úÖ ÁõÆÊ†á {target_name}: CV{score_type} = {np.mean(scores):.4f} ¬± {np.std(scores):.4f}"
                )

        self.is_trained = len(self.models) > 0

    # ÂÖ∂‰ªñÊñπÊ≥ï‰øùÊåÅ‰∏çÂèòÔºå‰ΩÜ‰ΩøÁî®‰ºòÂåñÂêéÁöÑÁâπÂæÅÂíåÊ®°Âûã
    def apply_enhanced_adjustment(self, df, predictions):
        """Â∫îÁî®Â¢ûÂº∫Ë∞ÉÊï¥ - ‰ΩøÁî®Êõ¥Â§öÊäÄÊúØÊåáÊ†á"""
        # ÂÆûÁé∞‰∏é‰πãÂâçÁ±ª‰ººÁöÑË∞ÉÊï¥ÈÄªËæëÔºå‰ΩÜ‰ΩøÁî®Êõ¥Â§öÁâπÂæÅ
        # ËøôÈáåÁúÅÁï•ËØ¶ÁªÜÂÆûÁé∞‰ª•ËäÇÁúÅÁ©∫Èó¥
        current_data = df.iloc[-1]
        current_close = current_data["Close"]

        # ËÆ°ÁÆóÊ∂®Ë∑åÂÅú‰ª∑Ê†º
        limit_up = round(current_close * 1.1, 2)
        limit_down = round(current_close * 0.9, 2)

        # ËøôÈáåÂÆûÁé∞Â¢ûÂº∫ÁöÑ‰ø°Âè∑ÂàÜÊûêÈÄªËæë
        # ‰ΩøÁî®Êõ¥Â§öÊäÄÊúØÊåáÊ†áËøõË°åÁªºÂêàÂà§Êñ≠

        return predictions, 0, 0, 0  # ËøîÂõûË∞ÉÊï¥ÂêéÁöÑÈ¢ÑÊµãÂíå‰ø°Âè∑ËÆ°Êï∞

    def predict_optimized(self, df):
        """‰ºòÂåñÈ¢ÑÊµã"""
        if not self.is_trained:
            raise ValueError("Ê®°ÂûãÊú™ËÆ≠ÁªÉÔºåËØ∑ÂÖàË∞ÉÁî®train_optimized_modelsÊñπÊ≥ï")

        # ÂáÜÂ§áÁâπÂæÅ
        features = self.prepare_optimized_features(df)
        latest_features = features.iloc[-1:].copy()

        predictions = {}

        for target_name, model_info in self.models.items():
            model = model_info["model"]
            scaler = model_info["scaler"]
            imputer = model_info["imputer"]
            feature_selector = model_info["feature_selector"]

            try:
                # Â§ÑÁêÜÁº∫Â§±ÂÄº
                X_imputed = imputer.transform(latest_features)

                # ÁâπÂæÅÈÄâÊã©
                X_selected = feature_selector.transform(X_imputed)

                # ÁâπÂæÅÊ†áÂáÜÂåñ
                X_scaled = scaler.transform(X_selected)

                is_classification = target_name in [
                    "target_next_next_up",
                    "target_next_up",
                    "target_up_1pct",
                    "target_up_2pct",
                    "target_up_3pct",
                    "target_up_5pct",
                    "target_up_7pct",
                    "target_up_10pct",
                    "target_up_15pct",
                    "target_down_1pct",
                    "target_down_2pct",
                    "target_down_3pct",
                    "target_down_5pct",
                    "target_down_7pct",
                    "target_down_10pct",
                    "target_down_15pct",
                    "target_limit_up",
                    "target_limit_up_next_next",
                    "target_consecutive_limit",
                ]

                if is_classification:
                    # ÂàÜÁ±ªÈ¢ÑÊµã
                    if hasattr(model, "predict_proba"):
                        proba = model.predict_proba(X_scaled)
                        if proba.shape[1] > 1:
                            predictions[target_name] = proba[0, 1]
                        else:
                            predictions[target_name] = 0.5
                    else:
                        pred = model.predict(X_scaled)
                        predictions[target_name] = pred[0] if len(pred) > 0 else 0.5
                else:
                    # ÂõûÂΩíÈ¢ÑÊµã
                    pred = model.predict(X_scaled)
                    current_price = df["Close"].iloc[-1]

                    if "high" in target_name:
                        base_pred = max(pred[0], current_price * 1.04)
                    elif "low" in target_name:
                        base_pred = max(pred[0], current_price * 0.96)
                    else:
                        base_pred = pred[0]

                    predictions[target_name] = max(0.01, float(base_pred))

            except Exception as e:
                print(f"È¢ÑÊµãÈîôËØØ {target_name}: {e}")
                # Êô∫ËÉΩÂõûÈÄÄ
                current_price = df["Close"].iloc[-1]
                if is_classification:
                    predictions[target_name] = 0.5
                elif "low" in target_name:
                    predictions[target_name] = current_price * 0.95
                else:
                    predictions[target_name] = current_price * 1.06

        # Á°Æ‰øùÊ†∏ÂøÉÈ¢ÑÊµãÁõÆÊ†áÈÉΩÂ≠òÂú®
        current_price = df["Close"].iloc[-1]
        core_targets = {
            "target_next_low": current_price * 0.95,
            "target_next_next_high": current_price * 1.05,
            "target_next_next_low": current_price * 0.93,
            "target_next_next_up": 0.5,
        }

        for target, default_value in core_targets.items():
            if target not in predictions:
                predictions[target] = default_value
                print(f"‚ö†Ô∏è  ‰ΩøÁî®ÈªòËÆ§ÂÄºÂ°´ÂÖÖÁº∫Â§±ÁöÑÁõÆÊ†á: {target} = {default_value}")

        # Â∫îÁî®Â¢ûÂº∫Ë∞ÉÊï¥
        predictions, total_bullish, strong_bullish, extreme_bullish = (
            self.apply_enhanced_adjustment(df, predictions)
        )

        # ËÆ°ÁÆóÂ¢ûÂº∫ÁΩÆ‰ø°Â∫¶
        confidence = {}
        for target_name in predictions:
            model_info = self.models.get(target_name, {})
            cv_score = model_info.get("cv_score", 0.5)

            # Âü∫Á°ÄÁΩÆ‰ø°Â∫¶
            is_classification = target_name in [
                "target_next_next_up",
                "target_next_up",
                "target_up_1pct",
                "target_up_2pct",
                "target_up_3pct",
                "target_up_5pct",
                "target_up_7pct",
                "target_up_10pct",
                "target_up_15pct",
                "target_down_1pct",
                "target_down_2pct",
                "target_down_3pct",
                "target_down_5pct",
                "target_down_7pct",
                "target_down_10pct",
                "target_down_15pct",
                "target_limit_up",
                "target_limit_up_next_next",
                "target_consecutive_limit",
            ]

            if is_classification:
                base_conf = max(0.5, min(0.95, cv_score))
            else:
                base_conf = max(0.6, min(0.92, cv_score))

            # Â¢ûÂº∫‰ø°Âè∑Âº∫Â∫¶Ë∞ÉÊï¥
            signal_boost = (
                0.12 * min(total_bullish, 15)
                + 0.08 * min(strong_bullish, 8)
                + 0.06 * min(extreme_bullish, 5)
            )

            # Ê®°ÂûãË¥®ÈáèË∞ÉÊï¥
            model_quality_boost = 0.0
            if cv_score > 0.8:
                model_quality_boost = 0.08
            elif cv_score > 0.7:
                model_quality_boost = 0.05
            elif cv_score > 0.6:
                model_quality_boost = 0.02

            confidence[target_name] = min(
                0.98, base_conf + signal_boost + model_quality_boost
            )

        return predictions, confidence, total_bullish, strong_bullish, extreme_bullish


def run_strategy_development(symbol, file_date):
    """
    ‰ºòÂåñÁâàÁ≠ñÁï•ÂºÄÂèëÂáΩÊï∞
    """
    file_path = f"output/{symbol}/{file_date}/data.csv"

    try:
        # ËØªÂèñÊï∞ÊçÆ
        df = pd.read_csv(file_path)
        df["Date"] = pd.to_datetime(df["Date"])
        df = df.sort_values("Date").reset_index(drop=True)

        print(f"üìä Âä†ËΩΩÊï∞ÊçÆ: {len(df)} Êù°ËÆ∞ÂΩï")
        print(f"üìÖ Êó∂Èó¥ËåÉÂõ¥: {df['Date'].min()} Âà∞ {df['Date'].max()}")

        # ÂàùÂßãÂåñ‰ºòÂåñÈ¢ÑÊµãÂô®
        predictor = OptimizedStockPredictor()

        # ËÆ°ÁÆóÊâ©Â±ïÁâπÂæÅ
        print("üîß ËÆ°ÁÆóÊâ©Â±ïÊäÄÊúØÊåáÊ†á...")
        df_features = predictor.create_expanded_features(df)

        # ÂàõÂª∫ÁõÆÊ†áÂèòÈáè
        df_targets = predictor.create_enhanced_targets(df_features)

        # ÂáÜÂ§áÁâπÂæÅ
        X = predictor.prepare_optimized_features(df_targets)

        # ÂáÜÂ§áÁõÆÊ†áÂèòÈáè
        targets = {
            "target_next_low": df_targets["target_next_low"],
            "target_next_next_high": df_targets["target_next_next_high"],
            "target_next_next_low": df_targets["target_next_next_low"],
            "target_next_next_up": df_targets["target_next_next_up"],
            "target_up_5pct": df_targets["target_up_5pct"],
            "target_up_7pct": df_targets["target_up_7pct"],
            "target_up_10pct": df_targets["target_up_10pct"],
            "target_limit_up": df_targets["target_limit_up"],
        }

        # Ê∏ÖÁêÜÊï∞ÊçÆ
        valid_mask = ~(X.isna().any(axis=1))
        for target in targets.values():
            valid_mask = valid_mask & ~target.isna()

        X_clean = X[valid_mask]
        targets_clean = {}
        for name, target in targets.items():
            targets_clean[name] = target[valid_mask]

        print(f"üßπ Ê∏ÖÊ¥óÂêéÊúâÊïàÊï∞ÊçÆ: {len(X_clean)} Êù°")
        print(f"üîç ÁâπÂæÅÊï∞Èáè: {X_clean.shape[1]}")

        if len(X_clean) < 100:
            raise ValueError("Êï∞ÊçÆÈáè‰∏çË∂≥ÔºåËá≥Â∞ëÈúÄË¶Å100‰∏™ÊúâÊïà‰∫§ÊòìÊó•Êï∞ÊçÆ")

        # ËÆ≠ÁªÉ‰ºòÂåñÊ®°Âûã
        predictor.train_optimized_models(X_clean, targets_clean)

        if not predictor.is_trained:
            raise ValueError("Ê®°ÂûãËÆ≠ÁªÉÂ§±Ë¥•")

        # ËøõË°å‰ºòÂåñÈ¢ÑÊµã
        print("üéØ ËøõË°å‰ºòÂåñÈ¢ÑÊµã...")
        predictions, confidence, total_bullish, strong_bullish, extreme_bullish = (
            predictor.predict_optimized(df_targets)
        )

        # ËæìÂá∫‰ºòÂåñÊä•Âëä
        current_price = df["Close"].iloc[-1]
        current_date = df["Date"].iloc[-1]

        print("\n" + "=" * 80)
        print(f"üèÜ ËÇ°Á•® {symbol} ‰ºòÂåñÂàÜÊûêÊä•Âëä")
        print("=" * 80)
        print(f"üìÖ ÂΩìÂâçÊó•Êúü: {current_date}")
        print(f"üí∞ ÂΩìÂâçÊî∂Áõò‰ª∑: {current_price:.2f}")
        print(
            f"üìà ‰ø°Âè∑ÂàÜÊûê: Âü∫Á°Ä{total_bullish - strong_bullish - extreme_bullish}‰∏™, Âº∫{strong_bullish}‰∏™, ÊûÅÁ´Ø{extreme_bullish}‰∏™, ÊÄªËÆ°{total_bullish}‰∏™"
        )

        print(f"\nüìä Ê†∏ÂøÉÈ¢ÑÊµãÁªìÊûú:")

        # ‰ΩøÁî®ÂÆâÂÖ®ÁöÑÂ≠óÂÖ∏ËÆøÈóÆÊñπÂºè
        print(
            f"  üîΩ ‰∏ã‰∏Ä‰∏™‰∫§ÊòìÊó•ÊúÄ‰Ωé‰ª∑: {predictions.get('target_next_low', current_price * 0.95):.2f}"
        )
        print(f"    ÁΩÆ‰ø°Â∫¶: {confidence.get('target_next_low', 0.5):.1%}")

        print(
            f"  üîº ‰∏ã‰∏ã‰∏™‰∫§ÊòìÊó•ÊúÄÈ´ò‰ª∑: {predictions.get('target_next_next_high', current_price * 1.05):.2f}"
        )
        print(f"    ÁΩÆ‰ø°Â∫¶: {confidence.get('target_next_next_high', 0.5):.1%}")

        print(
            f"  üîΩ ‰∏ã‰∏ã‰∏™‰∫§ÊòìÊó•ÊúÄ‰Ωé‰ª∑: {predictions.get('target_next_next_low', current_price * 0.93):.2f}"
        )
        print(f"    ÁΩÆ‰ø°Â∫¶: {confidence.get('target_next_next_low', 0.5):.1%}")

        print(
            f"  üìà ‰∏ã‰∏ã‰∏™‰∫§ÊòìÊó•‰∏äÊ∂®Ê¶ÇÁéá: {predictions.get('target_next_next_up', 0.5):.1%}"
        )
        print(f"    ÁΩÆ‰ø°Â∫¶: {confidence.get('target_next_next_up', 0.5):.1%}")

        # È¢ùÂ§ñÈ¢ÑÊµã
        if "target_up_5pct" in predictions:
            print(
                f"  ‚ö° ‰∏ã‰∏Ä‰∏™‰∫§ÊòìÊó•Â§ßÊ∂®(>5%)Ê¶ÇÁéá: {predictions['target_up_5pct']:.1%}"
            )
        if "target_up_7pct" in predictions:
            print(
                f"  ‚ö° ‰∏ã‰∏Ä‰∏™‰∫§ÊòìÊó•Â§ßÊ∂®(>7%)Ê¶ÇÁéá: {predictions['target_up_7pct']:.1%}"
            )
        if "target_up_10pct" in predictions:
            print(
                f"  ‚ö° ‰∏ã‰∏Ä‰∏™‰∫§ÊòìÊó•Â§ßÊ∂®(>10%)Ê¶ÇÁéá: {predictions['target_up_10pct']:.1%}"
            )
        if "target_limit_up" in predictions:
            print(f"  üöÄ ‰∏ã‰∏Ä‰∏™‰∫§ÊòìÊó•Ê∂®ÂÅúÊ¶ÇÁéá: {predictions['target_limit_up']:.1%}")

        # ÊäÄÊúØÂàÜÊûêÊëòË¶Å
        current_data = df_targets.iloc[-1]
        print(f"\nüîç ÊäÄÊúØÂàÜÊûêÊëòË¶Å:")
        print(
            f"  RSI(6/14/21): {current_data.get('rsi_6', 0):.1f}/{current_data.get('rsi_14', 0):.1f}/{current_data.get('rsi_21', 0):.1f}"
        )
        print(f"  MACD: {current_data.get('macd', 0):.4f}")
        print(f"  ADX: {current_data.get('adx_14', 0):.1f}")
        print(f"  Êàê‰∫§ÈáèÊØîÁéá: {current_data.get('volume_ratio_5', 0):.2f}x")
        print(f"  ËøûÁª≠Ê∂®ÂÅú: {current_data.get('consecutive_limit_up', 0)}Â§©")

        # ‰ª∑Ê†ºÁõÆÊ†áÂàÜÊûê
        next_next_high = predictions.get("target_next_next_high", current_price * 1.05)
        upside_potential = (next_next_high - current_price) / current_price * 100

        print(f"\nüéØ ‰ª∑Ê†ºÁõÆÊ†áÂàÜÊûê:")
        print(f"  ÁõÆÊ†áÊúÄÈ´ò‰ª∑: {next_next_high:.2f}")
        print(f"  ‰∏äÊ∂®ÊΩúÂäõ: {upside_potential:+.1f}%")

        # ‰ºòÂåñ‰∫§ÊòìÂª∫ËÆÆ
        up_prob = predictions.get("target_next_next_up", 0.5)
        limit_up_prob = predictions.get("target_limit_up", 0)
        big_up_7pct_prob = predictions.get("target_up_7pct", 0)

        print(f"\nüí° ‰ºòÂåñ‰∫§ÊòìÂª∫ËÆÆ:")
        if limit_up_prob > 0.4:
            print(f"  üöÄ ÊûÅÈ´òÊ∂®ÂÅúÊ¶ÇÁéá({limit_up_prob:.1%})ÔºåÂº∫ÁÉà‰π∞ÂÖ•‰ø°Âè∑!")
        elif big_up_7pct_prob > 0.6:
            print(
                f"  üî• È´òÂ§ßÊ∂®Ê¶ÇÁéá({big_up_7pct_prob:.1%})ÔºåÁõÆÊ†áÊ∂®ÂπÖ{upside_potential:+.1f}%ÔºåÂº∫ÁÉàÂª∫ËÆÆ‰π∞ÂÖ•"
            )
        elif total_bullish >= 20:
            print(
                f"  üü¢ ÊûÅÁ´ØÁúãÊ∂®‰ø°Âè∑Ôºå‰∏äÊ∂®Ê¶ÇÁéá{up_prob:.1%}ÔºåÁõÆÊ†áÊ∂®ÂπÖ{upside_potential:+.1f}%ÔºåÂº∫ÁÉàÂª∫ËÆÆ‰π∞ÂÖ•"
            )
        elif total_bullish >= 15:
            print(
                f"  üü¢ ÂæàÂº∫ÁúãÊ∂®‰ø°Âè∑Ôºå‰∏äÊ∂®Ê¶ÇÁéá{up_prob:.1%}ÔºåÁõÆÊ†áÊ∂®ÂπÖ{upside_potential:+.1f}%ÔºåÂª∫ËÆÆ‰π∞ÂÖ•"
            )
        elif total_bullish >= 10:
            print(
                f"  üü¢ Âº∫ÁúãÊ∂®‰ø°Âè∑Ôºå‰∏äÊ∂®Ê¶ÇÁéá{up_prob:.1%}ÔºåÁõÆÊ†áÊ∂®ÂπÖ{upside_potential:+.1f}%ÔºåÂª∫ËÆÆ‰π∞ÂÖ•"
            )
        elif total_bullish >= 5:
            print(f"  üü° ‰∏≠Á≠âÁúãÊ∂®‰ø°Âè∑ÔºåÂèØËÄÉËôëËΩª‰ªìÂèÇ‰∏é")
        else:
            print(f"  üî¥ ÁúãÊ∂®‰ø°Âè∑‰∏çË∂≥ÔºåÂª∫ËÆÆËßÑÈÅø")

        # ÂáÜÁ°ÆÁéáËØÑ‰º∞
        avg_confidence = np.mean(list(confidence.values())) if confidence else 0.5
        expected_accuracy = min(0.95, avg_confidence * 1.2)

        print(f"\nüìä È¢ÑÊµãÂáÜÁ°ÆÁéáËØÑ‰º∞:")
        print(f"  Âπ≥ÂùáÁΩÆ‰ø°Â∫¶: {avg_confidence:.1%}")
        print(f"  È¢ÑÊúüÂáÜÁ°ÆÁéá: {expected_accuracy:.1%}")

        if expected_accuracy > 0.85:
            print(f"  ‚úÖ ÊûÅÈ´òÂáÜÁ°ÆÁéáÈ¢ÑÊµãÔºåÂèØÈù†ÊÄßÂæàÈ´ò")
        elif expected_accuracy > 0.75:
            print(f"  ‚úÖ È´òÂáÜÁ°ÆÁéáÈ¢ÑÊµãÔºåÂèØÈù†ÊÄßËæÉÈ´ò")
        elif expected_accuracy > 0.65:
            print(f"  üìà ‰∏≠Á≠âÂáÜÁ°ÆÁéáÈ¢ÑÊµãÔºåÊúâ‰∏ÄÂÆöÂèÇËÄÉ‰ª∑ÂÄº")
        else:
            print(f"  ‚ö†Ô∏è  ÂáÜÁ°ÆÁéáËæÉ‰ΩéÔºåÂª∫ËÆÆË∞®ÊÖéÂèÇËÄÉ")

        # ËøîÂõûÂÆåÊï¥ÁªìÊûú
        result = {
            "symbol": symbol,
            "current_date": str(current_date),
            "current_price": float(current_price),
            "predictions": predictions,
            "confidence": confidence,
            "avg_confidence": float(avg_confidence),
            "expected_accuracy": float(expected_accuracy),
            "data_points": len(X_clean),
            "feature_count": X_clean.shape[1],
            "optimized": True,
        }

        return str(result)

    except Exception as e:
        print(f"‚ùå ÈîôËØØ: {str(e)}")
        import traceback

        traceback.print_exc()
        return None


# # Á§∫‰æãË∞ÉÁî®
# if __name__ == "__main__":
#     result = run_strategy_development("600977", "2024-01-15")

#     if result:
#         print(f"\n‚úÖ ‰ºòÂåñÈ¢ÑÊµãÂÆåÊàê!")
#         print(f"üìä ‰ΩøÁî®Êï∞ÊçÆ: {result['data_points']} ‰∏™‰∫§ÊòìÊó•")
#         print(f"üîç ÁâπÂæÅÊï∞Èáè: {result['feature_count']}")
#         print(f"üéØ È¢ÑÊúüÂáÜÁ°ÆÁéá: {result['expected_accuracy']:.1%}")
#     else:
#         print("‚ùå È¢ÑÊµãÂ§±Ë¥•!")
